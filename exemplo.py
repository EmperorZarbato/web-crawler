#!/usr/bin/env python3
"""
Exemplo Simples de Uso do Web Crawler
Para casos onde voc√™ n√£o precisa da interface gr√°fica
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from web_crawler import WebCrawler
import json

def exemplo_basico():
    """Exemplo b√°sico de crawling"""
    print("üï∑Ô∏è Exemplo B√°sico de Web Crawling")
    print("=" * 50)
    
    # Cria inst√¢ncia do crawler
    crawler = WebCrawler()
    
    # Configura sess√£o b√°sica
    crawler.setup_session(delay=1.0, timeout=10)
    
    # URLs para testar
    urls = [
        "https://httpbin.org/html",
        "https://quotes.toscrape.com",
        "https://example.com"
    ]
    
    print(f"Fazendo crawling de {len(urls)} URLs...\n")
    
    # Faz crawling
    results = crawler.crawl_multiple_urls(urls)
    
    # Mostra resultados
    print(f"\n‚úÖ Crawling conclu√≠do! {len(results)} p√°ginas processadas.")
    
    for i, result in enumerate(results, 1):
        print(f"\n--- Resultado {i} ---")
        print(f"URL: {result.url}")
        print(f"T√≠tulo: {result.title}")
        print(f"Descri√ß√£o: {result.description[:100]}..." if result.description else "Descri√ß√£o: N/A")
        print(f"Status: {result.status_code}")
        print(f"Tempo: {result.response_time:.2f}s")
        print(f"Links encontrados: {len(result.links)}")
        print(f"Imagens encontradas: {len(result.images)}")
    
    # Estat√≠sticas
    stats = crawler.get_statistics()
    print(f"\nüìä Estat√≠sticas:")
    print(f"Total de p√°ginas: {stats['total_pages']}")
    print(f"Tempo m√©dio: {stats['average_response_time']}s")
    print(f"Total de links: {stats['total_links_found']}")
    print(f"Total de imagens: {stats['total_images_found']}")
    
    # Exporta resultados
    print(f"\nüíæ Exportando resultados...")
    crawler.export_results("exemplo_resultados.xlsx", "excel")
    crawler.export_results("exemplo_resultados.csv", "csv")
    crawler.export_results("exemplo_resultados.json", "json")
    print("Arquivos exportados: exemplo_resultados.xlsx, .csv, .json")

def exemplo_avancado():
    """Exemplo com filtros e seletores personalizados"""
    print("\nüîß Exemplo Avan√ßado com Filtros")
    print("=" * 50)
    
    crawler = WebCrawler()
    
    # Headers personalizados
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        "Accept-Language": "pt-BR,pt;q=0.9,en;q=0.8"
    }
    
    crawler.setup_session(headers=headers, delay=0.5)
    
    # Seletores CSS personalizados
    selectors = {
        'title': ['h1', 'title', '.title'],
        'description': ['meta[name="description"]', '.description'],
        'content': ['article', '.content', 'main', 'p'],
        'links': ['a[href]'],
        'images': ['img[src]']
    }
    
    # Filtros de conte√∫do
    filters = {
        'keywords': ['python', 'web', 'quotes'],  # Deve conter pelo menos uma
        'min_length': 50,  # Conte√∫do m√≠nimo
        'exclude_keywords': ['spam', 'ads']  # Excluir se contiver
    }
    
    # URL de exemplo
    url = "https://quotes.toscrape.com"
    
    print(f"Fazendo crawling de: {url}")
    print(f"Com seletores: {selectors}")
    print(f"Com filtros: {filters}")
    
    result = crawler.crawl_url(
        url, 
        selectors=selectors, 
        content_filters=filters,
        respect_robots=True
    )
    
    if result:
        print(f"\n‚úÖ Sucesso!")
        print(f"T√≠tulo: {result.title}")
        print(f"Conte√∫do (primeiros 200 chars): {result.content[:200]}...")
        print(f"Links: {len(result.links)}")
        print(f"Imagens: {len(result.images)}")
    else:
        print(f"\n‚ùå Nenhum resultado (possivelmente filtrado)")

def exemplo_selenium():
    """Exemplo usando Selenium para sites com JavaScript"""
    print("\nüåê Exemplo com Selenium (JavaScript)")
    print("=" * 50)
    
    try:
        crawler = WebCrawler()
        
        # URL que requer JavaScript
        url = "https://quotes.toscrape.com/js/"
        
        print(f"Fazendo crawling com Selenium: {url}")
        print("Aguardando carregamento do JavaScript...")
        
        # JavaScript personalizado para executar
        js_code = "window.scrollTo(0, document.body.scrollHeight);"
        
        result = crawler.crawl_with_selenium(
            url, 
            wait_time=3,
            execute_js=js_code
        )
        
        if result:
            print(f"\n‚úÖ Crawling com JavaScript bem-sucedido!")
            print(f"T√≠tulo: {result.title}")
            print(f"Conte√∫do encontrado: {len(result.content)} caracteres")
            print(f"Links: {len(result.links)}")
        else:
            print(f"\n‚ùå Falha no crawling com Selenium")
            
    except Exception as e:
        print(f"\n‚ö†Ô∏è Erro no Selenium: {e}")
        print("Certifique-se de ter o Chrome instalado e selenium configurado")

if __name__ == "__main__":
    try:
        # Executa exemplos
        exemplo_basico()
        exemplo_avancado()
        exemplo_selenium()
        
        print(f"\nüéâ Todos os exemplos executados!")
        print(f"Verifique os arquivos de resultado gerados.")
        print(f"\nPara usar a interface gr√°fica, execute: python main.py")
        
    except KeyboardInterrupt:
        print(f"\n\n‚èπÔ∏è Execu√ß√£o interrompida pelo usu√°rio")
    except Exception as e:
        print(f"\n‚ùå Erro: {e}")
        print(f"Certifique-se de instalar as depend√™ncias: pip install -r requirements.txt")
