# Web Crawler Pro ğŸ•·ï¸

Uma aplicaÃ§Ã£o completa de Web Crawler desenvolvida em Python com interface grÃ¡fica moderna e recursos avanÃ§ados.

## âœ¨ CaracterÃ­sticas

### ğŸ¯ Interface GrÃ¡fica Moderna
- Interface desenvolvida com CustomTkinter
- Design responsivo e intuitivo
- Abas organizadas por funcionalidade
- Tema escuro moderno

### ğŸ”§ Recursos de Crawling
- **HTTP Requests**: Crawling bÃ¡sico com requests + BeautifulSoup
- **JavaScript Support**: Selenium para sites que usam JavaScript
- **MÃºltiplas URLs**: Processamento em lote
- **Delay configurÃ¡vel**: Controle de velocidade entre requisiÃ§Ãµes
- **Headers personalizados**: User-Agent e headers HTTP customizÃ¡veis
- **Proxy Support**: Suporte a proxies HTTP/HTTPS
- **Robots.txt**: Respeita automaticamente o robots.txt

### ğŸ›ï¸ Filtros e Seletores
- **Seletores CSS**: ConfiguraÃ§Ã£o personalizÃ¡vel para extraÃ§Ã£o
- **Filtros de conteÃºdo**: Por palavras-chave, tamanho, regex
- **Presets**: ConfiguraÃ§Ãµes prontas para e-commerce, blogs, redes sociais
- **ExclusÃ£o de conteÃºdo**: Filtros para remover spam/ads

### ğŸ“Š Resultados e ExportaÃ§Ã£o
- **VisualizaÃ§Ã£o em tabela**: Resultados organizados e navegÃ¡veis
- **EstatÃ­sticas detalhadas**: MÃ©tricas de performance
- **ExportaÃ§Ã£o mÃºltipla**: Excel, CSV, JSON
- **Sistema de logs**: Rastreamento completo de atividades

## ğŸš€ InstalaÃ§Ã£o

### 1. Clone ou baixe o projeto
```bash
git clone <url-do-repositorio>
cd Web\ Crawler
```

### 2. Instale as dependÃªncias
```bash
pip install -r requirements.txt
```

### 3. Execute a aplicaÃ§Ã£o
```bash
# Interface grÃ¡fica
python main.py

# Exemplo via linha de comando
python exemplo.py
```

## ğŸ“¦ DependÃªncias

- `requests` - HTTP requests
- `beautifulsoup4` - Parsing HTML
- `selenium` - JavaScript rendering
- `pandas` - ManipulaÃ§Ã£o de dados
- `customtkinter` - Interface grÃ¡fica moderna
- `fake-useragent` - User agents aleatÃ³rios
- `webdriver-manager` - Gerenciamento automÃ¡tico do ChromeDriver
- `openpyxl` - ExportaÃ§Ã£o Excel
- `lxml` - Parser XML/HTML rÃ¡pido

## ğŸ® Como Usar

### Interface GrÃ¡fica

1. **ConfiguraÃ§Ã£o BÃ¡sica**
   - Insira as URLs que deseja fazer crawling
   - Configure delay, timeout e User-Agent
   - Escolha entre crawling normal ou com Selenium

2. **ConfiguraÃ§Ãµes AvanÃ§adas**
   - Headers HTTP personalizados
   - ConfiguraÃ§Ãµes de proxy
   - JavaScript personalizado para Selenium

3. **Filtros e Seletores**
   - Configure seletores CSS para extraÃ§Ã£o especÃ­fica
   - Defina filtros por palavras-chave
   - Use presets para diferentes tipos de sites

4. **Execute e Visualize**
   - Clique em "Iniciar Crawling"
   - Acompanhe o progresso em tempo real
   - Visualize resultados e estatÃ­sticas
   - Exporte em diferentes formatos

### Uso ProgramÃ¡tico

```python
from web_crawler import WebCrawler

# Cria crawler
crawler = WebCrawler()

# Configura sessÃ£o
crawler.setup_session(delay=1.0, timeout=10)

# Crawling simples
result = crawler.crawl_url("https://example.com")

# Crawling com filtros
selectors = {
    'title': ['h1', 'title'],
    'content': ['article', '.content']
}

filters = {
    'keywords': ['python', 'web'],
    'min_length': 100
}

result = crawler.crawl_url(
    "https://example.com",
    selectors=selectors,
    content_filters=filters
)

# Exporta resultados
crawler.export_results("resultados.xlsx", "excel")
```

## ğŸ—ï¸ Estrutura do Projeto

```
Web Crawler/
â”œâ”€â”€ main.py              # AplicaÃ§Ã£o principal (GUI)
â”œâ”€â”€ exemplo.py           # Exemplos de uso
â”œâ”€â”€ web_crawler.py       # Classe principal do crawler
â”œâ”€â”€ crawler_gui.py       # Interface grÃ¡fica
â”œâ”€â”€ requirements.txt     # DependÃªncias
â”œâ”€â”€ README.md           # Este arquivo
â”œâ”€â”€ crawler.log         # Logs (criado automaticamente)
â””â”€â”€ resultados/         # DiretÃ³rio para resultados (criado automaticamente)
```

## ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

### Seletores CSS Personalizados
```json
{
  "title": ["h1", ".title", "#title"],
  "description": ["meta[name='description']", ".summary"],
  "content": ["article", ".post-content", "main"],
  "links": ["a[href]"],
  "images": ["img[src]"]
}
```

### Filtros de ConteÃºdo
```json
{
  "keywords": ["python", "web", "crawling"],
  "exclude_keywords": ["spam", "ads"],
  "min_length": 100,
  "regex": "\\b(python|django|flask)\\b"
}
```

### Headers Personalizados
```json
{
  "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
  "Accept-Language": "pt-BR,pt;q=0.9,en;q=0.8",
  "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
}
```

## ğŸ¯ Presets IncluÃ­dos

### E-commerce
- Foco em produtos, preÃ§os e descriÃ§Ãµes
- Seletores para tÃ­tulos de produtos e especificaÃ§Ãµes
- Filtros para remover itens fora de estoque

### Blog/NotÃ­cias
- ExtraÃ§Ã£o de artigos e posts
- Seletores para tÃ­tulos, resumos e conteÃºdo principal
- Filtros para remover anÃºncios

### Redes Sociais
- Perfis e posts de redes sociais
- Seletores para bio, descriÃ§Ãµes e feeds
- Filtros mÃ­nimos para mÃ¡xima captura

## ğŸš€ Recursos Futuros

- [ ] Crawling distribuÃ­do
- [ ] API REST
- [ ] Mais formatos de exportaÃ§Ã£o
- [ ] Agendamento de crawls
- [ ] Dashboard web
- [ ] Suporte a mais navegadores (Firefox, Edge)
- [ ] IntegraÃ§Ã£o com bancos de dados

## ğŸ› SoluÃ§Ã£o de Problemas

### Erro de importaÃ§Ã£o do Selenium
```bash
pip install selenium
# Certifique-se de ter o Chrome instalado
```

### Erro de ChromeDriver
O webdriver-manager baixa automaticamente o ChromeDriver, mas certifique-se de ter o Chrome instalado.

### Erro de CustomTkinter
```bash
pip install customtkinter
```

### Sites que nÃ£o carregam
- Tente usar o modo Selenium para sites com JavaScript
- Verifique se o site permite crawling (robots.txt)
- Ajuste o delay entre requisiÃ§Ãµes

## ğŸ“„ LicenÃ§a

Este projeto Ã© open source. Use livremente para fins educacionais e comerciais.

## ğŸ¤ ContribuiÃ§Ãµes

ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se Ã  vontade para:
- Reportar bugs
- Sugerir melhorias
- Enviar pull requests

---

**Desenvolvido com â¤ï¸ em Python**

*Lembre-se de sempre respeitar o robots.txt dos sites e usar o crawler de forma Ã©tica!*
