# Web Crawler Pro üï∑Ô∏è

Uma aplica√ß√£o completa de Web Crawler desenvolvida em Python com interface gr√°fica moderna e recursos avan√ßados.

## Caracter√≠sticas

### Interface Gr√°fica Moderna
- Interface desenvolvida com CustomTkinter
- Design responsivo e intuitivo
- Abas organizadas por funcionalidade
- Tema escuro moderno

### Recursos de Crawling
- **HTTP Requests**: Crawling b√°sico com requests + BeautifulSoup
- **JavaScript Support**: Selenium para sites que usam JavaScript
- **M√∫ltiplas URLs**: Processamento em lote
- **Delay configur√°vel**: Controle de velocidade entre requisi√ß√µes
- **Headers personalizados**: User-Agent e headers HTTP customiz√°veis
- **Proxy Support**: Suporte a proxies HTTP/HTTPS
- **Robots.txt**: Respeita automaticamente o robots.txt

###  Filtros e Seletores
- **Seletores CSS**: Configura√ß√£o personaliz√°vel para extra√ß√£o
- **Filtros de conte√∫do**: Por palavras-chave, tamanho, regex
- **Presets**: Configura√ß√µes prontas para e-commerce, blogs, redes sociais
- **Exclus√£o de conte√∫do**: Filtros para remover spam/ads

### Resultados e Exporta√ß√£o
- **Visualiza√ß√£o em tabela**: Resultados organizados e naveg√°veis
- **Estat√≠sticas detalhadas**: M√©tricas de performance
- **Exporta√ß√£o m√∫ltipla**: Excel, CSV, JSON
- **Sistema de logs**: Rastreamento completo de atividades

## Instala√ß√£o

### 1. Clone ou baixe o projeto
```bash
git clone <url-do-repositorio>
cd Web\ Crawler
```

### 2. Instale as depend√™ncias
```bash
pip install -r requirements.txt
```

### 3. Execute a aplica√ß√£o
```bash
# Interface gr√°fica
python main.py

# Exemplo via linha de comando
python exemplo.py
```

## Depend√™ncias

- `requests` - HTTP requests
- `beautifulsoup4` - Parsing HTML
- `selenium` - JavaScript rendering
- `pandas` - Manipula√ß√£o de dados
- `customtkinter` - Interface gr√°fica moderna
- `fake-useragent` - User agents aleat√≥rios
- `webdriver-manager` - Gerenciamento autom√°tico do ChromeDriver
- `openpyxl` - Exporta√ß√£o Excel
- `lxml` - Parser XML/HTML r√°pido

## Como Usar

### Interface Gr√°fica

1. **Configura√ß√£o B√°sica**
   - Insira as URLs que deseja fazer crawling
   - Configure delay, timeout e User-Agent
   - Escolha entre crawling normal ou com Selenium

2. **Configura√ß√µes Avan√ßadas**
   - Headers HTTP personalizados
   - Configura√ß√µes de proxy
   - JavaScript personalizado para Selenium

3. **Filtros e Seletores**
   - Configure seletores CSS para extra√ß√£o espec√≠fica
   - Defina filtros por palavras-chave
   - Use presets para diferentes tipos de sites

4. **Execute e Visualize**
   - Clique em "Iniciar Crawling"
   - Acompanhe o progresso em tempo real
   - Visualize resultados e estat√≠sticas
   - Exporte em diferentes formatos

### Uso Program√°tico

```python
from web_crawler import WebCrawler

# Cria crawler
crawler = WebCrawler()

# Configura sess√£o
crawler.setup_session(delay=1.0, timeout=10)

# Crawling simples
result = crawler.crawl_url("https://example.com")

# Crawling com filtros
selectors = {
    'title': ['h1', 'title'],
    'content': ['article', '.content']
}

filters = {
    'keywords': ['python', 'web'],
    'min_length': 100
}

result = crawler.crawl_url(
    "https://example.com",
    selectors=selectors,
    content_filters=filters
)

# Exporta resultados
crawler.export_results("resultados.xlsx", "excel")
```

## Estrutura do Projeto

```
Web Crawler/
‚îú‚îÄ‚îÄ main.py              # Aplica√ß√£o principal (GUI)
‚îú‚îÄ‚îÄ exemplo.py           # Exemplos de uso
‚îú‚îÄ‚îÄ web_crawler.py       # Classe principal do crawler
‚îú‚îÄ‚îÄ crawler_gui.py       # Interface gr√°fica
‚îú‚îÄ‚îÄ requirements.txt     # Depend√™ncias
‚îú‚îÄ‚îÄ README.md           # Este arquivo
‚îú‚îÄ‚îÄ crawler.log         # Logs (criado automaticamente)
‚îî‚îÄ‚îÄ resultados/         # Diret√≥rio para resultados (criado automaticamente)
```

## Configura√ß√µes Avan√ßadas

### Seletores CSS Personalizados
```json
{
  "title": ["h1", ".title", "#title"],
  "description": ["meta[name='description']", ".summary"],
  "content": ["article", ".post-content", "main"],
  "links": ["a[href]"],
  "images": ["img[src]"]
}
```

### Filtros de Conte√∫do
```json
{
  "keywords": ["python", "web", "crawling"],
  "exclude_keywords": ["spam", "ads"],
  "min_length": 100,
  "regex": "\\b(python|django|flask)\\b"
}
```

### Headers Personalizados
```json
{
  "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
  "Accept-Language": "pt-BR,pt;q=0.9,en;q=0.8",
  "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
}
```

## Presets Inclu√≠dos

### E-commerce
- Foco em produtos, pre√ßos e descri√ß√µes
- Seletores para t√≠tulos de produtos e especifica√ß√µes
- Filtros para remover itens fora de estoque

### Blog/Not√≠cias
- Extra√ß√£o de artigos e posts
- Seletores para t√≠tulos, resumos e conte√∫do principal
- Filtros para remover an√∫ncios

### Redes Sociais
- Perfis e posts de redes sociais
- Seletores para bio, descri√ß√µes e feeds
- Filtros m√≠nimos para m√°xima captura

## Solu√ß√£o de Problemas

### Erro de importa√ß√£o do Selenium
```bash
pip install selenium
# Certifique-se de ter o Chrome instalado
```

### Erro de ChromeDriver
O webdriver-manager baixa automaticamente o ChromeDriver, mas certifique-se de ter o Chrome instalado.

### Erro de CustomTkinter
```bash
pip install customtkinter
```

### Sites que n√£o carregam
- Tente usar o modo Selenium para sites com JavaScript
- Verifique se o site permite crawling (robots.txt)
- Ajuste o delay entre requisi√ß√µes

## Licen√ßa

Este projeto √© open source. Use livremente para fins educacionais e comerciais.

## Contribui√ß√µes

Contribui√ß√µes s√£o bem-vindas! Sinta-se √† vontade para:
- Reportar bugs
- Sugerir melhorias
- Enviar pull requests

---

*Lembre-se de sempre respeitar o robots.txt dos sites e usar o crawler de forma √©tica!*

