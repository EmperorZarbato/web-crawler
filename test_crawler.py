#!/usr/bin/env python3
"""
Script de Teste para Web Crawler Pro
Testa todas as funcionalidades principais
"""

import sys
import os
import time
import json
from datetime import datetime

# Adiciona o diret√≥rio atual ao path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_imports():
    """Testa se todas as importa√ß√µes est√£o funcionando"""
    print("üß™ Testando importa√ß√µes...")
    
    try:
        import requests
        print("‚úì requests")
    except ImportError as e:
        print(f"‚ùå requests: {e}")
        return False
    
    try:
        from bs4 import BeautifulSoup
        print("‚úì beautifulsoup4")
    except ImportError as e:
        print(f"‚ùå beautifulsoup4: {e}")
        return False
    
    try:
        import pandas as pd
        print("‚úì pandas")
    except ImportError as e:
        print(f"‚ùå pandas: {e}")
        return False
    
    try:
        import customtkinter as ctk
        print("‚úì customtkinter")
    except ImportError as e:
        print(f"‚ùå customtkinter: {e}")
        return False
    
    try:
        from selenium import webdriver
        print("‚úì selenium")
    except ImportError as e:
        print(f"‚ùå selenium: {e}")
        return False
    
    try:
        from fake_useragent import UserAgent
        print("‚úì fake-useragent")
    except ImportError as e:
        print(f"‚ùå fake-useragent: {e}")
        return False
    
    try:
        from webdriver_manager.chrome import ChromeDriverManager
        print("‚úì webdriver-manager")
    except ImportError as e:
        print(f"‚ùå webdriver-manager: {e}")
        return False
    
    return True

def test_basic_crawler():
    """Testa o crawler b√°sico"""
    print("\nüß™ Testando crawler b√°sico...")
    
    try:
        from web_crawler import WebCrawler
        
        crawler = WebCrawler()
        crawler.setup_session(delay=0.5, timeout=5)
        
        # Testa uma URL simples
        test_url = "https://httpbin.org/html"
        print(f"Testando URL: {test_url}")
        
        result = crawler.crawl_url(test_url)
        
        if result:
            print("‚úì Crawling b√°sico funcionando")
            print(f"  - T√≠tulo: {result.title[:50]}...")
            print(f"  - Status: {result.status_code}")
            print(f"  - Tempo: {result.response_time:.2f}s")
            return True
        else:
            print("‚ùå Crawling b√°sico falhou")
            return False
            
    except Exception as e:
        print(f"‚ùå Erro no teste b√°sico: {e}")
        return False

def test_selectors():
    """Testa seletores CSS"""
    print("\nüß™ Testando seletores CSS...")
    
    try:
        from web_crawler import WebCrawler
        
        crawler = WebCrawler()
        crawler.setup_session(delay=0.5, timeout=5)
        
        # Seletores personalizados
        selectors = {
            'title': ['title', 'h1'],
            'content': ['p', 'div'],
            'links': ['a[href]']
        }
        
        test_url = "https://httpbin.org/html"
        result = crawler.crawl_url(test_url, selectors=selectors)
        
        if result:
            print("‚úì Seletores CSS funcionando")
            print(f"  - Links encontrados: {len(result.links)}")
            return True
        else:
            print("‚ùå Seletores CSS falharam")
            return False
            
    except Exception as e:
        print(f"‚ùå Erro no teste de seletores: {e}")
        return False

def test_filters():
    """Testa filtros de conte√∫do"""
    print("\nüß™ Testando filtros de conte√∫do...")
    
    try:
        from web_crawler import WebCrawler
        
        crawler = WebCrawler()
        crawler.setup_session(delay=0.5, timeout=5)
        
        # Filtros que devem passar
        filters_pass = {
            'keywords': ['html', 'test'],
            'min_length': 10
        }
        
        # Filtros que devem falhar
        filters_fail = {
            'keywords': ['palavranaoexiste'],
            'min_length': 10000
        }
        
        test_url = "https://httpbin.org/html"
        
        # Teste que deve passar
        result1 = crawler.crawl_url(test_url, content_filters=filters_pass)
        
        # Limpa resultados para pr√≥ximo teste
        crawler.results = []
        crawler.visited_urls = set()
        
        # Teste que deve falhar
        result2 = crawler.crawl_url(test_url, content_filters=filters_fail)
        
        if result1 and not result2:
            print("‚úì Filtros de conte√∫do funcionando")
            return True
        else:
            print("‚ùå Filtros de conte√∫do falharam")
            print(f"  - Teste 1 (deve passar): {'‚úì' if result1 else '‚ùå'}")
            print(f"  - Teste 2 (deve falhar): {'‚úì' if not result2 else '‚ùå'}")
            return False
            
    except Exception as e:
        print(f"‚ùå Erro no teste de filtros: {e}")
        return False

def test_export():
    """Testa exporta√ß√£o de resultados"""
    print("\nüß™ Testando exporta√ß√£o...")
    
    try:
        from web_crawler import WebCrawler
        
        crawler = WebCrawler()
        crawler.setup_session(delay=0.5, timeout=5)
        
        # Faz crawling para ter dados
        test_url = "https://httpbin.org/html"
        result = crawler.crawl_url(test_url)
        
        if not result:
            print("‚ùå Falha ao obter dados para teste de exporta√ß√£o")
            return False
        
        # Testa exporta√ß√£o
        test_files = {
            'excel': 'teste_resultados.xlsx',
            'csv': 'teste_resultados.csv',
            'json': 'teste_resultados.json'
        }
        
        for format_type, filename in test_files.items():
            try:
                crawler.export_results(filename, format_type)
                if os.path.exists(filename):
                    print(f"‚úì Exporta√ß√£o {format_type.upper()} funcionando")
                    os.remove(filename)  # Limpa arquivo de teste
                else:
                    print(f"‚ùå Arquivo {format_type.upper()} n√£o foi criado")
                    return False
            except Exception as e:
                print(f"‚ùå Erro na exporta√ß√£o {format_type.upper()}: {e}")
                return False
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erro no teste de exporta√ß√£o: {e}")
        return False

def test_config():
    """Testa carregamento de configura√ß√£o"""
    print("\nüß™ Testando configura√ß√µes...")
    
    try:
        config_file = "config.json"
        if os.path.exists(config_file):
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # Verifica estrutura b√°sica
            required_keys = ['app_name', 'version', 'default_settings', 'presets']
            for key in required_keys:
                if key not in config:
                    print(f"‚ùå Configura√ß√£o inv√°lida: falta chave '{key}'")
                    return False
            
            print("‚úì Configura√ß√µes carregadas corretamente")
            print(f"  - App: {config['app_name']} v{config['version']}")
            print(f"  - Presets dispon√≠veis: {len(config['presets'])}")
            return True
        else:
            print("‚ùå Arquivo config.json n√£o encontrado")
            return False
            
    except Exception as e:
        print(f"‚ùå Erro no teste de configura√ß√£o: {e}")
        return False

def test_selenium():
    """Testa Selenium (opcional)"""
    print("\nüß™ Testando Selenium...")
    
    try:
        from web_crawler import WebCrawler
        from selenium.webdriver.chrome.options import Options
        
        crawler = WebCrawler()
        
        # Tenta um crawling simples com Selenium
        test_url = "https://httpbin.org/html"
        print(f"Testando Selenium com: {test_url}")
        print("(Isso pode demorar um pouco...)")
        
        result = crawler.crawl_with_selenium(test_url, wait_time=2)
        
        if result:
            print("‚úì Selenium funcionando")
            print(f"  - T√≠tulo: {result.title[:50]}...")
            print(f"  - Tempo: {result.response_time:.2f}s")
            return True
        else:
            print("‚ùå Selenium falhou")
            return False
            
    except Exception as e:
        print(f"‚ö†Ô∏è Selenium n√£o dispon√≠vel: {e}")
        print("  (Isso √© normal se o Chrome n√£o estiver instalado)")
        return True  # N√£o falha o teste geral

def run_all_tests():
    """Executa todos os testes"""
    print("üß™ WEB CRAWLER PRO - SUITE DE TESTES")
    print("=" * 50)
    print(f"Data/Hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    tests = [
        ("Importa√ß√µes", test_imports),
        ("Crawler B√°sico", test_basic_crawler),
        ("Seletores CSS", test_selectors),
        ("Filtros", test_filters),
        ("Exporta√ß√£o", test_export),
        ("Configura√ß√µes", test_config),
        ("Selenium", test_selenium)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        print(f"\n--- {test_name} ---")
        start_time = time.time()
        
        try:
            success = test_func()
            duration = time.time() - start_time
            results.append((test_name, success, duration))
            
        except Exception as e:
            duration = time.time() - start_time
            print(f"‚ùå Erro inesperado: {e}")
            results.append((test_name, False, duration))
    
    # Relat√≥rio final
    print("\n" + "=" * 50)
    print("üìä RELAT√ìRIO FINAL DOS TESTES")
    print("=" * 50)
    
    passed = 0
    total = len(results)
    
    for test_name, success, duration in results:
        status = "‚úì PASSOU" if success else "‚ùå FALHOU"
        print(f"{test_name:20} | {status} | {duration:.2f}s")
        if success:
            passed += 1
    
    print("-" * 50)
    print(f"Total: {passed}/{total} testes passaram ({(passed/total)*100:.1f}%)")
    
    if passed == total:
        print("\nüéâ TODOS OS TESTES PASSARAM!")
        print("O Web Crawler est√° funcionando corretamente.")
    elif passed >= total * 0.8:
        print("\n‚ö†Ô∏è MAIORIA DOS TESTES PASSOU")
        print("O Web Crawler deve funcionar, mas verifique os erros acima.")
    else:
        print("\n‚ùå MUITOS TESTES FALHARAM")
        print("Verifique a instala√ß√£o das depend√™ncias:")
        print("pip install -r requirements.txt")
    
    print(f"\nPara executar o Web Crawler:")
    print(f"  Interface Gr√°fica: python main.py")
    print(f"  Exemplos: python exemplo.py")

if __name__ == "__main__":
    try:
        run_all_tests()
    except KeyboardInterrupt:
        print(f"\n\n‚èπÔ∏è Testes interrompidos pelo usu√°rio")
    except Exception as e:
        print(f"\n‚ùå Erro fatal: {e}")
    
    print(f"\nPressione Enter para continuar...")
    input()
